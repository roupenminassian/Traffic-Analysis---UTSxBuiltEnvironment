{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python matplotlib transformers timm torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Initializing object detection model...\")\n",
    "\n",
    "# Load the DETR model and feature extractor for vehicles\n",
    "extractor_vehicle = AutoFeatureExtractor.from_pretrained(\"roupenminassian/detr-resnet-101-finetuned\")\n",
    "model_vehicle = AutoModelForObjectDetection.from_pretrained(\"roupenminassian/detr-resnet-101-finetuned\")\n",
    "\n",
    "# Load the DETR model and feature extractor for pedestrians\n",
    "extractor_pedestrian = AutoFeatureExtractor.from_pretrained(\"roupenminassian/detr-resnet-101-finetuned-pedestrains\")\n",
    "model_pedestrian = AutoModelForObjectDetection.from_pretrained(\"roupenminassian/detr-resnet-101-finetuned-pedestrains\")\n",
    "\n",
    "# Move the models to the selected device\n",
    "model_vehicle = model_vehicle.to(device)\n",
    "model_pedestrian = model_pedestrian.to(device)\n",
    "\n",
    "print(\"Model initialized!\")\n",
    "\n",
    "# COCO Class labels mapping for vehicles\n",
    "COCO_LABELS_VEHICLE = {0: 'Car', 1: 'Van', 2: 'Truck', 3: 'Bus', 4: 'Motorbike'}\n",
    "\n",
    "# COCO Class labels mapping for pedestrians\n",
    "COCO_LABELS_PEDESTRIAN = {0: 'Pedestrian'}\n",
    "\n",
    "def map_indices_to_labels_vehicle(index):\n",
    "    return COCO_LABELS_VEHICLE.get(index, None)\n",
    "\n",
    "def map_indices_to_labels_pedestrian(index):\n",
    "    return COCO_LABELS_PEDESTRIAN.get(index, None)\n",
    "\n",
    "print(\"Setting up video capture...\")\n",
    "# Load the video for processing\n",
    "video_path = \"D:\\Downloads\\GOPR0069 (2).MP4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Total frames in the video: {total_frames}\")\n",
    "height, width = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# Active objects will be stored in this list\n",
    "active_objects = []\n",
    "\n",
    "# Global counter for unique IDs\n",
    "object_id_counter = 1\n",
    "\n",
    "# Define points for perspective transformation\n",
    "src_points = np.array([\n",
    "    [274, 720],\n",
    "    [575, 348],\n",
    "    [789, 348],\n",
    "    [1089, 720]\n",
    "], dtype=np.float32)\n",
    "\n",
    "dst_width = width\n",
    "dst_height = int(height * 0.75)  # Adjust this to control the height of the bird's-eye view\n",
    "dst_points = np.array([\n",
    "    [0, dst_height],\n",
    "    [0, 0],\n",
    "    [dst_width, 0],\n",
    "    [dst_width, dst_height]\n",
    "], dtype=np.float32)\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "\n",
    "# Warp the source points using the transformation matrix\n",
    "warped_src_points = cv2.perspectiveTransform(src_points.reshape(-1, 1, 2), M).reshape(-1, 2)\n",
    "\n",
    "# Calculate the pixel distance between the top and bottom points in the transformed frame\n",
    "warped_distance_pixels = np.linalg.norm(warped_src_points[0] - warped_src_points[3])\n",
    "\n",
    "# Define lane separation and x-coordinates for lanes\n",
    "line_separation = dst_width / 5  # Divided by 5 to get 4 lanes\n",
    "center_x = dst_width / 2 - 25\n",
    "x_coords = [\n",
    "    center_x - 1.5 * line_separation,\n",
    "    center_x - 0.5 * line_separation,\n",
    "    center_x + 0.5 * line_separation,\n",
    "    center_x + 1.5 * line_separation\n",
    "]\n",
    "\n",
    "def get_lane_from_centroid(cx):\n",
    "    if cx < x_coords[0]:\n",
    "        return \"Lane 1\"\n",
    "    elif cx < x_coords[1]:\n",
    "        return \"Lane 2\"\n",
    "    elif cx < x_coords[2]:\n",
    "        return \"Lane 3\"\n",
    "    elif cx < x_coords[3]:\n",
    "        return \"Lane 4\"\n",
    "    else:\n",
    "        return \"Lane 5\"\n",
    "    \n",
    "# Function to calculate IoU between two bounding boxes\n",
    "def compute_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "    \n",
    "    xi1, yi1, xi2, yi2 = max(x1, x1_), max(y1, y1_), min(x2, x2_), min(y2, y2_)\n",
    "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "    \n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "    \n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "def get_video_creation_date(video_path):\n",
    "    cmd = ['exiftool', '-s', '-s', '-s', '-CreateDate', video_path]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        return result.stdout.strip()\n",
    "    else:\n",
    "        print(f\"Error executing exiftool: {result.stderr}\")\n",
    "        return None\n",
    "\n",
    "print(\"Fetching video creation date...\")\n",
    "creation_date_str = get_video_creation_date(video_path)\n",
    "creation_date = datetime.strptime(creation_date_str, '%Y:%m:%d %H:%M:%S')\n",
    "print(f\"Video creation date: {creation_date}\")\n",
    "\n",
    "all_labels = list(COCO_LABELS.values())\n",
    "all_lanes = [\"Lane 1\", \"Lane 2\", \"Lane 3\", \"Lane 4\", \"Lane 5\"]\n",
    "\n",
    "detections = []\n",
    "pedestrian_count_list = []\n",
    "previous_detections = []\n",
    "object_speeds = {}  # New line: Dictionary to store object speeds\n",
    "frame_count = 0\n",
    "conversion_factor = 1\n",
    "\n",
    "MAX_PERMISSIBLE_DISTANCE = 50\n",
    "REAL_WORLD_DISTANCE = 50 # in meters\n",
    "REAL_WORLD_BOX_HEIGHT = 50 # in meters\n",
    "\n",
    "# Calculate the pixel-to-meter conversion factor\n",
    "conversion_factor = REAL_WORLD_BOX_HEIGHT / warped_distance_pixels\n",
    "\n",
    "# Define the number of segments\n",
    "NUM_SEGMENTS = 10\n",
    "segment_height = dst_height / NUM_SEGMENTS\n",
    "\n",
    "# Define the pixel-to-meter ratio at the bottom and top of the image\n",
    "# You can adjust these based on your knowledge of the scene\n",
    "ratio_bottom = REAL_WORLD_DISTANCE / dst_height\n",
    "ratio_top = ratio_bottom * 0.5  # This is an example; adjust as needed\n",
    "\n",
    "# Calculate the ratio for each segment\n",
    "segment_ratios = np.linspace(ratio_bottom, ratio_top, NUM_SEGMENTS)\n",
    "\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_count = 0\n",
    "\n",
    "print(\"Starting frame processing...\")\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Failed to read frame {frame_count}. Ending processing.\")\n",
    "        break\n",
    "\n",
    "    timestamp_seconds = frame_count / frame_rate\n",
    "    current_timestamp = creation_date + timedelta(seconds=timestamp_seconds)\n",
    "    print(f\"\\nProcessing frame {frame_count} at timestamp {current_timestamp}...\")\n",
    "\n",
    "    # Transform the frame to bird's-eye view\n",
    "    warped_frame = cv2.warpPerspective(frame, M, (dst_width, dst_height))\n",
    "\n",
    "    current_detections = []\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(warped_frame, cv2.COLOR_BGR2RGB)\n",
    "    inputs = extractor_vehicle(images=frame_rgb, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model_vehicle(**inputs)\n",
    "\n",
    "    predicted_logits = outputs.logits.softmax(-1).detach().cpu().numpy()\n",
    "    predicted_boxes = outputs.pred_boxes[0].detach().cpu().numpy()\n",
    "\n",
    "    # Create a temporary dictionary to store the counts for this frame\n",
    "    frame_counts = {(label, lane): 0 for label in all_labels for lane in all_lanes}\n",
    "\n",
    "    for query in range(predicted_logits.shape[1]):\n",
    "        label = predicted_logits[0, query].argmax()\n",
    "        label_name = map_indices_to_labels_vehicle(label)\n",
    "\n",
    "        if label_name and predicted_logits[0, query][label] > 0.6 and label_name in COCO_LABELS.values():\n",
    "            center_x = int(predicted_boxes[query, 0] * dst_width)\n",
    "            center_y = int(predicted_boxes[query, 1] * dst_height)\n",
    "            box_width = int(predicted_boxes[query, 2] * dst_width)\n",
    "            box_height = int(predicted_boxes[query, 3] * dst_height)\n",
    "\n",
    "            xmin = int(center_x - box_width / 2)\n",
    "            ymin = int(center_y - box_height / 2)\n",
    "            xmax = int(center_x + box_width / 2)\n",
    "            ymax = int(center_y + box_height / 2)\n",
    "\n",
    "            current_detections.append({\n",
    "            'label': label_name,\n",
    "            'centroid': (center_x, center_y),\n",
    "            'box': (xmin, ymin, xmax, ymax)\n",
    "            })\n",
    "\n",
    "            cv2.rectangle(warped_frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "            cv2.putText(warped_frame, f\"{label_name}: {predicted_logits[0, query][label]:.2f}\", (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "            lane_of_object = get_lane_from_centroid(center_x)\n",
    "            print(f\"Object {label_name} at ({center_x}, {center_y}) is in: {lane_of_object}\")\n",
    "\n",
    "    # Tracking logic\n",
    "    for curr_det in current_detections:\n",
    "        max_iou = -1\n",
    "        best_match = None\n",
    "        for active_obj in active_objects:\n",
    "            iou = compute_iou(curr_det['box'], active_obj['bbox'])\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "                best_match = active_obj\n",
    "\n",
    "        if best_match and max_iou > 0.5:\n",
    "            best_match['bbox'] = curr_det['box']\n",
    "            best_match['last_seen'] = 0\n",
    "            curr_det['id'] = best_match['id']  # Assign the id of the best match to the current detection\n",
    "        else:\n",
    "            active_objects.append({\n",
    "                'id': object_id_counter,\n",
    "                'bbox': curr_det['box'],\n",
    "                'last_seen': 0\n",
    "            })\n",
    "            curr_det['id'] = object_id_counter  # Assign the new id to the current detection\n",
    "            object_id_counter += 1 \n",
    "\n",
    "    for active_obj in active_objects:\n",
    "        active_obj['last_seen'] += 1\n",
    "\n",
    "    active_objects = [obj for obj in active_objects if obj['last_seen'] < 5]\n",
    "\n",
    "    # Now, calculate speeds by comparing current detections with previous detections\n",
    "    for curr_det in current_detections:\n",
    "        closest_prev_det = None\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        for prev_det in previous_detections:\n",
    "            if prev_det['label'] == curr_det['label']:\n",
    "                distance = np.sqrt((curr_det['centroid'][0] - prev_det['centroid'][0])**2 + \n",
    "                                   (curr_det['centroid'][1] - prev_det['centroid'][1])**2)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_prev_det = prev_det\n",
    "\n",
    "        if closest_prev_det and min_distance <= MAX_PERMISSIBLE_DISTANCE:\n",
    "            # Calculate speed in pixels/frame\n",
    "            speed_pixel_per_frame = min_distance\n",
    "\n",
    "            # Convert this to real-world speed using frame rate and the conversion factor\n",
    "            speed_m_per_s = speed_pixel_per_frame * frame_rate * conversion_factor\n",
    "            speed_km_per_h = speed_m_per_s * 3.6  # Convert m/s to km/h\n",
    "\n",
    "            obj_id = curr_det['id']\n",
    "            if obj_id not in object_speeds:\n",
    "                object_speeds[obj_id] = []\n",
    "            object_speeds[obj_id].append(speed_km_per_h)\n",
    "           \n",
    "            print(f\"Object {curr_det['label']} in {get_lane_from_centroid(curr_det['centroid'][0])} moved at {speed_km_per_h:.2f} km/h\")\n",
    "\n",
    "        detections.append({\n",
    "        'timestamp': current_timestamp,\n",
    "        'label': curr_det['label'],\n",
    "        'lane': get_lane_from_centroid(curr_det['centroid'][0]),\n",
    "        'count': 1, # Since this is per object, the count is 1\n",
    "        'id': curr_det['id']\n",
    "    })\n",
    "\n",
    "    # Update previous detections for the next iteration\n",
    "    previous_detections = current_detections\n",
    "        \n",
    "    for x in x_coords:\n",
    "        cv2.line(warped_frame, (int(x), 0), (int(x), dst_height), (255, 0, 0), 2)\n",
    "\n",
    "    print(f\"Total number of detections: {len(detections)}\")\n",
    "\n",
    "    # Initialize pedestrian count for this frame\n",
    "    pedestrian_count = 0\n",
    "\n",
    "    # Process the frame using the pedestrian model\n",
    "    inputs_pedestrian = extractor_pedestrian(images=frame_rgb, return_tensors=\"pt\")\n",
    "    inputs_pedestrian = {key: val.to(device) for key, val in inputs_pedestrian.items()}\n",
    "    outputs_pedestrian = model_pedestrian(**inputs_pedestrian)\n",
    "\n",
    "    pedestrian_logits = outputs_pedestrian.logits.softmax(-1).detach().cpu().numpy()\n",
    "\n",
    "    for query in range(pedestrian_logits.shape[1]):\n",
    "        label = pedestrian_logits[0, query].argmax()\n",
    "        label_name = map_indices_to_labels_pedestrian(label)\n",
    "        \n",
    "        if label_name and pedestrian_logits[0, query][label] > 0.6:\n",
    "            pedestrian_count += 1\n",
    "\n",
    "    pedestrian_count_list.append(pedestrian_count)\n",
    "\n",
    "    cv2.imshow('Bird\\'s-Eye View with Lanes and Detected Objects', warped_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"User interrupted video processing!\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "print(\"Releasing video capture and closing windows...\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of detections to a DataFrame\n",
    "df = pd.DataFrame(detections)\n",
    "\n",
    "# Round off the timestamp to the nearest second\n",
    "df['rounded_timestamp'] = df['timestamp'].dt.floor('s')\n",
    "\n",
    "# New code for filtering and speed calculations:\n",
    "threshold = 0.05 * frame_rate  # 5% of frames within a second\n",
    "df['average_speed'] = df['id'].apply(lambda x: np.median(object_speeds.get(x, [0])))\n",
    "\n",
    "print(\"Aggregating data...\")\n",
    "# Aggregate by timestamp, label, lane, and ID\n",
    "aggregated_data = df.groupby(['rounded_timestamp', 'label', 'lane', 'id']).agg({\n",
    "    'average_speed': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Convert the data to the desired format\n",
    "formatted_data = []\n",
    "for timestamp, timestamp_group in aggregated_data.groupby('rounded_timestamp'):\n",
    "    entry = {'TIME series': timestamp}\n",
    "    for _, row in timestamp_group.iterrows():\n",
    "        lane = row['lane']\n",
    "        if f'{lane}_objects' not in entry:\n",
    "            entry[f'{lane}_objects'] = []\n",
    "        object_details = {\n",
    "            'id': row['id'],\n",
    "            'type': row['label'],\n",
    "            'speed': row['average_speed']\n",
    "        }\n",
    "        entry[f'{lane}_objects'].append(object_details)\n",
    "    formatted_data.append(entry)\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame(formatted_data)\n",
    "\n",
    "# Rename the rounded_timestamp back to timestamp for clarity\n",
    "final_df.rename(columns={'rounded_timestamp': 'timestamp'}, inplace=True)\n",
    "\n",
    "# Convert pedestrian_count_list into a DataFrame\n",
    "df_pedestrians = pd.DataFrame({\n",
    "    'timestamp': [entry['timestamp'] for entry in detections],\n",
    "    'pedestrian_count': pedestrian_count_list\n",
    "})\n",
    "\n",
    "# Aggregate to get the average pedestrian count per second\n",
    "aggregated_pedestrians = df_pedestrians.groupby(df_pedestrians['timestamp'].dt.floor('s')).agg({\n",
    "    'pedestrian_count': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge this data with the final_df on the timestamp\n",
    "final_df = pd.merge(final_df, aggregated_pedestrians, left_on='TIME series', right_on='timestamp', how='left').drop(columns=['timestamp'])\n",
    "\n",
    "# Rename the pedestrian_count column to avg_pedestrian_count for clarity\n",
    "final_df.rename(columns={'pedestrian_count': 'avg_pedestrian_count'}, inplace=True)\n",
    "\n",
    "print(final_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
